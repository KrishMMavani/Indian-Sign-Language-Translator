{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553b5c7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train features are 0\n",
      "length of test features are 0\n",
      "length of all train discriptors is 0\n",
      "Mini batch K-Means started.\n",
      "0 descriptors before clustering\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MiniBatchKMeans.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 197\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_disc_by_class, test_disc_by_class\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m### STEP:2 MINI K-MEANS\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m mini_kmeans_model \u001b[38;5;241m=\u001b[39m mini_kmeans(ipu\u001b[38;5;241m.\u001b[39mN_CLASSES \u001b[38;5;241m*\u001b[39m ipu\u001b[38;5;241m.\u001b[39mCLUSTER_FACTOR, np\u001b[38;5;241m.\u001b[39marray(all_train_dis))\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m all_train_dis\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m### Collecting VISUAL WORDS for all images (train , test)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 127\u001b[0m, in \u001b[0;36mmini_kmeans\u001b[1;34m(k, descriptor_list)\u001b[0m\n\u001b[0;32m    125\u001b[0m descriptor_list \u001b[38;5;241m=\u001b[39m descriptor_list\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    126\u001b[0m kmeans_model \u001b[38;5;241m=\u001b[39m MiniBatchKMeans(k)\n\u001b[1;32m--> 127\u001b[0m kmeans_model\u001b[38;5;241m.\u001b[39mfit(descriptor_list)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMini batch K means trained to get visual words.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    129\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_kmeans_model.sav\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:2080\u001b[0m, in \u001b[0;36mMiniBatchKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the centroids on X by chunking it into mini-batches.\u001b[39;00m\n\u001b[0;32m   2055\u001b[0m \n\u001b[0;32m   2056\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2080\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   2081\u001b[0m         X,\n\u001b[0;32m   2082\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2083\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[0;32m   2084\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2085\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2086\u001b[0m     )\n\u001b[0;32m   2088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   2089\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:969\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    970\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    973\u001b[0m         )\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    976\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MiniBatchKMeans."
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as skmetrics\n",
    "import random\n",
    "import pickle\n",
    "import imagePreprocessingUtils as ipu\n",
    "from imagePreprocessingUtils import get_ORB_descriptors\n",
    "\n",
    "#import glob\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "def preprocess_all_images():\n",
    "    images_labels = []\n",
    "    train_disc_by_class = {}\n",
    "    test_disc_by_class = {}\n",
    "    all_train_dis = []\n",
    "    train_img_disc = []\n",
    "    test_img_disc = []\n",
    "    label_value = 0\n",
    "    for (dirpath,dirnames,filenames) in os.walk(ipu.PATH):\n",
    "        dirnames.sort()\n",
    "        for label in dirnames:\n",
    "            #print(label)\n",
    "            if not (label == '.DS_Store'):\n",
    "                for (subdirpath,subdirnames,images) in os.walk(ipu.PATH+'/'+label+'/'):\n",
    "                    #print(len(images))\n",
    "                    count = 0\n",
    "                    train_features = []\n",
    "                    test_features = []\n",
    "                    for image in images: \n",
    "                        #print(label)\n",
    "                        imagePath = ipu.PATH+'/'+label+'/'+image\n",
    "                        #print(imagePath)\n",
    "                        img = cv2.imread(imagePath)\n",
    "                        if img is not None:\n",
    "                            img = get_canny_edge(img)[0]\n",
    "                            orb_disc = get_ORB_descriptors(img)\n",
    "                            print(orb_disc.shape)\n",
    "                            if(count < (ipu.TOTAL_IMAGES * ipu.TRAIN_FACTOR * 0.01)):\n",
    "                                print('Train:--------- Label is {} and Count is {}'.format(label, count)  )\n",
    "                                #train_features.append(orb_disc)\n",
    "                                train_img_disc.append(orb_disc)\n",
    "                                all_train_dis.extend(orb_disc)\n",
    "                                train_labels.append(label_value)\n",
    "                            elif((count>=(ipu.TOTAL_IMAGES * ipu.TRAIN_FACTOR  * 0.01)) and count <ipu.TOTAL_IMAGES):\n",
    "                                print('Test:--------- Label is {} and Count is {}'.format(label, count)  )\n",
    "                                #test_features.append(orb_disc)\n",
    "                                test_img_disc.append(orb_disc)\n",
    "                                test_labels.append(label_value)\n",
    "                            count += 1\n",
    "                        #images_labels.append((label,orb_disc))\n",
    "                #train_disc_by_class[label] = train_features\n",
    "                #test_disc_by_class[label] = test_features\n",
    "                label_value +=1\n",
    "    print('length of train features are %i' % len(train_img_disc))\n",
    "    print('length of test features are %i' % len(test_img_disc))\n",
    "    print('length of all train discriptors is {}'.format(len(all_train_dis)))\n",
    "    #print('length of all train discriptors by class  is {}'.format(len(train_disc_by_class)))\n",
    "    #print('length of all test disc is {}'.format(len(test_disc_by_class))) \n",
    "    return all_train_dis, train_img_disc, train_disc_by_class, test_disc_by_class, test_img_disc\n",
    "\n",
    "\n",
    "\n",
    "def get_canny_edge(image):\n",
    "    grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Convert from RGB to HSV\n",
    "    HSVImaage = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) \n",
    "\n",
    "    # Finding pixels with itensity of skin\n",
    "    lowerBoundary = np.array([0,40,30],dtype=\"uint8\")\n",
    "    upperBoundary = np.array([43,255,254],dtype=\"uint8\")\n",
    "    skinMask = cv2.inRange(HSVImaage, lowerBoundary, upperBoundary)\n",
    "    # blurring of gray scale using medianBlur\n",
    "    skinMask = cv2.addWeighted(skinMask,0.5,skinMask,0.5,0.0)\n",
    "    skinMask = cv2.medianBlur(skinMask, 5)\n",
    "    skin = cv2.bitwise_and(grayImage, grayImage, mask = skinMask)\n",
    "    #cv2.imshow(\"masked2\",skin)\n",
    "    #. canny edge detection\n",
    "    canny = cv2.Canny(skin,60,60)\n",
    "    #plt.imshow(img2, cmap = 'gray')\n",
    "    return canny,skin\n",
    "\n",
    "def get_SIFT_descriptors(canny):\n",
    "    # Intialising SIFT\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    #surf.extended=True\n",
    "    canny = cv2.resize(canny,(256,256))\n",
    "    # computing SIFT descriptors\n",
    "    kp, des = surf.detectAndCompute(canny,None)\n",
    "    #print(len(des))\n",
    "    #sift_features_image = cv2.drawKeypoints(canny,kp,None,(0,0,255),4)\n",
    "    return des\n",
    "\n",
    "### K-means is not used as data is large and requires a better computer with good specifications\n",
    "def kmeans(k, descriptor_list):\n",
    "    print('K-Means started.')\n",
    "    print ('%i descriptors before clustering' % descriptor_list.shape[0])\n",
    "    kmeanss = KMeans(k)\n",
    "    kmeanss.fit(descriptor_list)\n",
    "    visual_words = kmeanss.cluster_centers_ \n",
    "    return visual_words, kmeanss\n",
    "\n",
    "# def mini_kmeans(k, descriptor_list):\n",
    "#     print('Mini batch K-Means started.')\n",
    "#     print ('%i descriptors before clustering' % descriptor_list.shape[0])\n",
    "#     kmeans_model = MiniBatchKMeans(k)\n",
    "#     kmeans_model.fit(descriptor_list)\n",
    "#     print('Mini batch K means trained to get visual words.')\n",
    "#     filename = 'mini_kmeans_model.sav'\n",
    "#     pickle.dump(kmeans_model, open(filename, 'wb'))\n",
    "#     return kmeans_model\n",
    "def mini_kmeans(k, descriptor_list):\n",
    "    print('Mini batch K-Means started.')\n",
    "    print ('%i descriptors before clustering' % descriptor_list.shape[0])\n",
    "    # Reshape the array to make it 2D\n",
    "    descriptor_list = descriptor_list.reshape(-1, 1)\n",
    "    kmeans_model = MiniBatchKMeans(k)\n",
    "    kmeans_model.fit(descriptor_list)\n",
    "    print('Mini batch K means trained to get visual words.')\n",
    "    filename = 'mini_kmeans_model.sav'\n",
    "    pickle.dump(kmeans_model, open(filename, 'wb'))\n",
    "    return kmeans_model\n",
    "\n",
    "\n",
    "def get_histograms(discriptors_by_class,visual_words, cluster_model):\n",
    "    histograms_by_class = {}\n",
    "    total_histograms = []\n",
    "    for label,images_discriptors in discriptors_by_class.items():\n",
    "        print('Label: %s' % label)\n",
    "        histograms = []\n",
    "        #    loop for all images \n",
    "        for each_image_discriptors in images_discriptors:\n",
    "            ## manual method to calculate words occurence as histograms\n",
    "            '''histogram = np.zeros(len(visual_words))\n",
    "            # loop for all discriptors in a image discriptorss \n",
    "            for each_discriptor in each_image_discriptors:\n",
    "                #list_words = visual_words.tolist()\n",
    "                a = np.array([visual_words])\n",
    "                index = find_index(each_discriptor, visual_words)\n",
    "                #print(index)\n",
    "                #del list_words\n",
    "                histogram[index] += 1\n",
    "            print(histogram)'''\n",
    "            ## using cluster model\n",
    "            raw_words = cluster_model.predict(each_image_discriptors)\n",
    "            hist =  np.bincount(raw_words, minlength=len(visual_words))\n",
    "            print(hist)\n",
    "            histograms.append(hist)\n",
    "        histograms_by_class[label] = histograms\n",
    "        total_histograms.append(histograms)\n",
    "    print('Histograms succesfully created for %i classes.' % len(histograms_by_class))\n",
    "    return histograms_by_class, total_histograms\n",
    "def dataSplit(dataDictionary):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for key,values in dataDictionary.items():\n",
    "        for value in values:\n",
    "            X.append(value)\n",
    "            Y.append(key)\n",
    "    return X,Y\n",
    "\n",
    "def predict_svm(X_train, X_test, y_train, y_test):\n",
    "    svc=SVC(kernel='linear')\n",
    "    print(\"Support Vector Machine started.\")\n",
    "    svc.fit(X_train,y_train)\n",
    "    filename = 'svm_model.sav'\n",
    "    pickle.dump(svc, open(filename, 'wb'))\n",
    "    y_pred=svc.predict(X_test)\n",
    "    np.savetxt('submission_svm.csv', np.c_[range(1,len(y_test)+1),y_pred,y_test], delimiter=',', header = 'ImageId,PredictedLabel,TrueLabel', comments = '', fmt='%d')\n",
    "    calculate_metrics(\"SVM\",y_test,y_pred)\n",
    "\n",
    "def calculate_metrics(method,label_test,label_pred):\n",
    "    print(\"Accuracy score for \",method,skmetrics.accuracy_score(label_test,label_pred))\n",
    "    print(\"Precision_score for \",method,skmetrics.precision_score(label_test,label_pred,average='micro'))\n",
    "    print(\"f1 score for \",method,skmetrics.f1_score(label_test,label_pred,average='micro'))\n",
    "    print(\"Recall score for \",method,skmetrics.recall_score(label_test,label_pred,average='micro'))\n",
    "\n",
    "\n",
    "### STEP:1 SIFT discriptors for all train and test images with class seperation\n",
    "\n",
    "all_train_dis,train_img_disc, train_disc_by_class, test_disc_by_class, test_img_disc  = preprocess_all_images()\n",
    "\n",
    "##  deleting these variables as they are not used with mini batch k means\n",
    "del train_disc_by_class, test_disc_by_class\n",
    "\n",
    "### STEP:2 MINI K-MEANS\n",
    "\n",
    "mini_kmeans_model = mini_kmeans(ipu.N_CLASSES * ipu.CLUSTER_FACTOR, np.array(all_train_dis))\n",
    "\n",
    "del all_train_dis\n",
    "\n",
    "### Collecting VISUAL WORDS for all images (train , test)\n",
    "\n",
    "print('Collecting visual words for train .....')\n",
    "train_images_visual_words = [mini_kmeans_model.predict(visual_words) for visual_words in train_img_disc]\n",
    "print('Visual words for train data collected. length is %i' % len(train_images_visual_words))\n",
    "\n",
    "print('Collecting visual words for test .....')\n",
    "test_images_visual_words = [mini_kmeans_model.predict(visual_words) for visual_words in test_img_disc]\n",
    "print('Visual words for test data collected. length is %i' % len(test_images_visual_words))\n",
    "\n",
    "\n",
    "### STEP:3 HISTOGRAMS (findiing the occurence of each visual word of images in total words)\n",
    "## Can be calculated using get_histograms function also manually\n",
    "\n",
    "print('Calculating Histograms for train...')\n",
    "bovw_train_histograms = np.array([np.bincount(visual_words, minlength=ipu.N_CLASSES * ipu.CLUSTER_FACTOR) for visual_words in train_images_visual_words])\n",
    "print('Train histograms are collected. Length : %i ' % len(bovw_train_histograms))\n",
    "\n",
    "print('Calculating Histograms for test...')\n",
    "bovw_test_histograms = np.array([np.bincount(visual_words, minlength=ipu.N_CLASSES * ipu.CLUSTER_FACTOR) for visual_words in test_images_visual_words])\n",
    "print('Test histograms are collected. Length : %i ' % len(bovw_test_histograms))\n",
    "\n",
    "print('Each histogram length is : %i' % len(bovw_train_histograms[0]))\n",
    "#----------------------\n",
    "print('============================================')\n",
    "\n",
    "# preperaing for training svm\n",
    "X_train = bovw_train_histograms\n",
    "X_test = bovw_test_histograms\n",
    "Y_train = train_labels\n",
    "Y_test = test_labels\n",
    "\n",
    "#print(Y_train)\n",
    "### shuffling \n",
    "\n",
    "buffer  = list(zip(X_train, Y_train))\n",
    "random.shuffle(buffer)\n",
    "random.shuffle(buffer)\n",
    "random.shuffle(buffer)\n",
    "X_train, Y_train = zip(*buffer)\n",
    "#print(Y_train)\n",
    "\n",
    "buffer  = list(zip(X_test, Y_test))\n",
    "random.shuffle(buffer)\n",
    "random.shuffle(buffer)\n",
    "X_test, Y_test = zip(*buffer)\n",
    "\n",
    "print('Length of X-train:  %i ' % len(X_train))\n",
    "print('Length of Y-train:  %i ' % len(Y_train))\n",
    "print('Length of X-test:  %i ' % len(X_test))\n",
    "print('Length of Y-test:  %i ' % len(Y_test))\n",
    "\n",
    "predict_svm(X_train, X_test,Y_train, Y_test)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "#STEP:2 K-MEANS clustering to get visual words \n",
    "\n",
    "visual_words, cluster_model = kmeans(ipu.N_CLASSES * 8, np.array(all_train_dis))\n",
    "\n",
    "print(' Length of Visual words using k-means= %i' % len(visual_words))\n",
    "print(type(visual_words))\n",
    "print(visual_words.shape)\n",
    "\n",
    "\n",
    "print('Histograms creation started for training set.')\n",
    "   \n",
    "bovw_train_histograms_by_class = get_histograms(train_disc_by_class,visual_words, cluster_model)[0]\n",
    "print('Histograms created with k-means.')\n",
    "\n",
    "\n",
    "for key, values in bovw_train_histograms_by_class.items():\n",
    "    for value in values:\n",
    "        print(value)\n",
    "   \n",
    "\n",
    "print('Histograms creation started for testing set.')\n",
    "bovw_test_histograms_by_class = get_histograms(test_disc_by_class,visual_words, cluster_model)[0]\n",
    "print('Histograms created.')\n",
    "\n",
    "X_train, Y_train = dataSplit(bovw_train_histograms_by_class)\n",
    "\n",
    "print('Length of x_train are % i ' % len(X_train))\n",
    "print('Length of y_train are % i ' % len(Y_train))\n",
    "\n",
    "X_test, Y_test = dataSplit(bovw_test_histograms_by_class)\n",
    "\n",
    "print('Length of x_test are % i ' % len(X_test))\n",
    "print('Length of y_test are % i ' % len(Y_test))\n",
    "\n",
    "\n",
    "X_train, Y_train = dataSplit(bovw_train_histograms_by_class)\n",
    "predict_svm(X_train, X_test,Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43b0006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516bddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
